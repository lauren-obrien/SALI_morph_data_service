---
title: "Morphology WFS - Option 1"
output: html_document
author: "Lauren O'Brien"
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results = 'hold', fig.height = 6,
                      fig.align = 'center', warning = FALSE, message = FALSE)
```

## Background

Experiments with extracting and formatting Queensland soils morphology data for WFS publication. Primary driver is the TERN SGLA II modelling (and particularly interface with the soil data federator https://github.com/RossDSearle/SoilDataFederatoR/) but the service should be useful to a wider range of clients as well.

```{r pkgs, results = 'hide'}
options(stringsAsFactors = FALSE, scipen = 999)
library(sf)            # vector data IO and processing
library(tidyverse)     # general data manipulation and display
```

### 'Simple' conversion

The simplest way to turn the morphology data into a service (from a maintainer's perspective, at least) is just to keep the existing layer structure from the SALI database, and spatialise each table. 

```{r datammmmm}
today <- '20191212' 
sit_datafile <- file.path(getwd(), 'SALI_DATA', 
                          paste0('SALI_SIT-data_', today, '.rds'))
sit_data <- readRDS(sit_datafile)
sit_data[['LAB']] <- NULL # out of scope still
```

Pros:

  * Layer structure is familiar to existing SALI users
  * Simple to extract and process data
  * Simple to write metadata
  * Minimises no-data cells in output layers
  
Cons: 

  * WFS doesn't easily support cross-layer querying - joining tables must be done client-side
  * Geometry repeats a lot - inefficient?
  * Site-level data doesn't have a point location. Options:
      * Create multipoint geometry from all observations on a site
      * Create polygon from bounding box or convex hull of observations
      * Use the location of observation one

Given that about 99.5% of sites have only one observation, the location of Observation 1 is used below to assign location to site-level data.

```{r spatialise_all, eval = FALSE}
spat_data <- sit_data
 
# not in use
spat_data[['SIT']] <- 
  dplyr::select(spat_data[['SIT']], -CONTEXT_PROJECT_CODE, -CONTEXT_SITE_ID)

locs <- spat_data[['OLC']] %>% 
  st_as_sf(coords = c('LONGITUDE', 'LATITUDE'), crs = 4283) %>% 
  dplyr::select(PROJECT_CODE, SITE_ID, OBS_NO)

locs_obs_1 <- dplyr::filter(locs, OBS_NO == 1) %>% dplyr::select(-OBS_NO)

spat_data[['OLC']] <- NULL # will be redundant

spat_data[1:5] <- 
  lapply(spat_data[1:5], function(lyr) {
    right_join(locs_obs_1, lyr, by = c('PROJECT_CODE', 'SITE_ID')) %>% 
      dplyr::select(-CREATED_BY, -LAST_UPDATED_BY)
    })

spat_data[6:length(spat_data)] <- 
  lapply(spat_data[6:length(spat_data)], function(lyr) {
    right_join(locs, lyr, by = c('PROJECT_CODE', 'SITE_ID', 'OBS_NO')) %>% 
      dplyr::select(-CREATED_BY, -LAST_UPDATED_BY)
    })

# rearrange for neat
spat_data[['NOT']] <- spat_data[['NOT']] %>% 
  dplyr::select(PROJECT_CODE, SITE_ID, OBS_NO, HORIZON_NO, everything())
# i can stop any time i want!

# Testing the build process in Arc highlights that Field test data really needs 
# to be spread by test_type and method - however, since some sites have replicate
# measurements, a REP_NO level needs to be added 
# most replicates are probably import errors but some appear to be genuine,
# so can't just take median or whatever
# argh i'm too stupid for this today
#fts_test <- sit_data[['FTS']] %>% 
#  group_by(PROJECT_CODE, SITE_ID, OBS_NO, HORIZON_NO, TEST_TYPE, METHOD, DEPTH) %>% 
#  mutate(TEST_NO =,
#         REP_NO = ) %>% 
#  
#  ungroup() %>% 
#  unite('TEST_TYPE_METHOD', TEST_TYPE, METHOD, sep = '_METHOD_', na.rm = TRUE) %>% 
#  pivot_wider(names_from = TEST_TYPE_METHOD, values_from = VALUE,
#              id_cols = c(PROJECT_CODE, SITE_ID, OBS_NO, HORIZON_NO, TEST_NO,DEPTH)) %>% 
#  dplyr::select(PROJECT_CODE, SITE_ID, OBS_NO, HORIZON_NO, DEPTH, matches('^PH'),
#                matches('^EC'), matches('^DISP'), matches('^SLAK'), everything()) %>% 
#  rename(H2O2EF = H2O2) %>% 
#  mutate_at(vars(matches('^PH|^EC')), as.numeric)


saveRDS(spat_data, file.path(getwd(), 'DERIVED_DATA', 'SALI_Morph_spatial.rds'))
# spat_data <- readRDS(file.path(getwd(), 'DERIVED_DATA', 'SALI_Morph_spatial.rds'))
```

All that geometry doubles the file size, yikes.

### Output

Writing to File GDB for use in Arc:

Note that using `reticulate` and `arcgisbridge` in the same session appears impossible at present; one locks up the software license and prevents the next from working :|

```{r create_gdb, eval = FALSE}
test_gdb <- file.path(getwd(), 'DERIVED_DATA', 'TEST_01_SIMPLE.gdb')

if(!(dir.exists(test_gdb))) {
  # launch access to arcpy
  library(reticulate)
  arcpy3_dir <- 'C:/Program Files/ArcGIS/Pro/bin/Python'
  system2(file.path(arcpy3_dir, 'Scripts', 'proenv.bat'))
  # make an empty file geodatabase
  arcpy <- import('arcpy')
  arcpy$CreateFileGDB_management(file.path(getwd(), 'DERIVED_DATA'), 
                                 'TEST_01_SIMPLE.gdb')
}

```

```{r write_gdb, eval = FALSE, echo = -1}
test_gdb <- file.path(getwd(), 'DERIVED_DATA', 'TEST_01_SIMPLE.gdb')
library(arcgisbinding)
arc.check_product()

# reserved term whoops
names(spat_data)[which(names(spat_data) == 'NOT')] <- 'NOTES'

purrr::map2(spat_data, names(spat_data), function(lyr, nm) {
  arc.write(file.path(test_gdb, nm), lyr, overwrite = TRUE)
})

```

```{r check_write, echo = -1}
test_gdb <- file.path(getwd(), 'DERIVED_DATA', 'TEST_01_SIMPLE.gdb')
sf::st_layers(test_gdb)
```

***
